{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('universal_tagset')\n",
    "brownwords = nltk.corpus.brown.tagged_words(categories='news', tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(brownwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brownwords[5425]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(tag for (word,tag) in brownwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of nouns:',fd['NOUN'])\n",
    "print('Number of adjectives:',fd['ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create our own tagger; start with some baselines!\n",
    "bad_tagger = nltk.DefaultTagger('NOUN')\n",
    "text = nltk.word_tokenize('It could be that it rained, or is raining heavily.')\n",
    "bad_tagger.tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk can evaluate the tagger, but we need it represented as tagged sentences:\n",
    "brownsentences = nltk.corpus.brown.tagged_sents(categories='news', tagset='universal')\n",
    "bad_tagger.evaluate(brownsentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rule based tagger!\n",
    "patterns = [\n",
    "    (r'.*ly$', 'ADV'),\n",
    "    (r'.*ing$', 'VERB'),\n",
    "    (r'.*ed$', 'VERB'),\n",
    "    (r'.*ould$', 'VERB'),\n",
    "    (r'^[.]$', 'PUNCT'),\n",
    "    (r'^[,]$', 'PUNCT'),\n",
    "    (r'.*$', 'NOUN'),\n",
    "]\n",
    "rule_tagger = nltk.RegexpTagger(patterns)\n",
    "rule_tagger.tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rule_tagger.evaluate(brownsentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(brownwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(cfd['the']['DET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfd['pay'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(cfd['pay']['NOUN'])\n",
    "print(cfd['pay']['VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justwords = nltk.FreqDist(nltk.corpus.brown.words(categories='news')).keys()\n",
    "best_tags = dict((w, cfd[w].max()) for w in justwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Most frequent tag for \"the\":', best_tags['the'])\n",
    "print('Most frequent tag for \"pay\":', best_tags['pay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split = int(len(brownsentences)*0.9)\n",
    "train = brownsentences[:split]\n",
    "test = brownsentences[split:]\n",
    "unigram_tagger = nltk.UnigramTagger(train)\n",
    "unigram_tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "better_tagger = nltk.UnigramTagger(train, backoff=rule_tagger)\n",
    "better_tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flipped = [(t,w) for sent in train for (w,t) in sent]\n",
    "wordgiventag = nltk.ConditionalFreqDist(flipped)\n",
    "# check same counts as above, but just training data:\n",
    "print(wordgiventag['DET']['the'])\n",
    "print(wordgiventag['VERB']['pay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is P(w|t), unsmoothed!\n",
    "def P(w,t):\n",
    "    return wordgiventag[t][w] / wordgiventag[t].N()\n",
    "\n",
    "print('P(the|DET) =', P('the','DET'))\n",
    "print('P(is|VERB) =', P('is','VERB'))\n",
    "print('P(pay|VERB) =', P('pay','VERB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_bigrams = [(x,y) for sent in train for x,y in nltk.bigrams([t for (w,t) in sent])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_bigrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_bigram_counts = nltk.ConditionalFreqDist(tag_bigrams)\n",
    "# this is count of noun tags following adjective tags (normal order in English)\n",
    "print(tag_bigram_counts['ADJ']['NOUN'])\n",
    "# this is count of adjective tags following noun tags\n",
    "print(tag_bigram_counts['NOUN']['ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is P(t2|t1), unsmoothed again!\n",
    "def tagP(t2,t1):\n",
    "    return tag_bigram_counts[t1][t2] / tag_bigram_counts[t1].N()\n",
    "print('P(NOUN|ADJ) =',tagP('NOUN','ADJ'))\n",
    "print('P(NOUN|DET) =',tagP('NOUN','DET'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentence_start = nltk.FreqDist(sent[0][1] for sent in train)\n",
    "def initP(t):\n",
    "    return sentence_start[t] / sentence_start.N()\n",
    "print('initP(DET) =', initP('DET'))\n",
    "print('initP(PRON) =', initP('PRON'))  # he, she, it, etc.\n",
    "print('initP(NOUN) =', initP('NOUN'))\n",
    "print('initP(VERB) =', initP('VERB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(V,tag_list,t,i):\n",
    "    ans=-1\n",
    "    best=None\n",
    "    for s in tag_list:\n",
    "        temp=V[(s,i-1)]*tagP(t,s)\n",
    "        if temp > ans:\n",
    "            ans = temp\n",
    "            best = s\n",
    "    return (best,ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printV(sentence,tag_list,V,B):\n",
    "    for i in range(len(sentence)):\n",
    "        print('i='+str(i)+' ['+sentence[i]+']')\n",
    "        for t in tag_list:\n",
    "            if V[(t,i)] != 0:\n",
    "                toprint='  '+t+'='+str(V[(t,i)])\n",
    "                if i>0:\n",
    "                    toprint += ' (from '+B[(t,i)]+')'\n",
    "                print(toprint)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence):\n",
    "    V = dict()    # keys are (t,i) where t is a tag (row label) and i is position in sentence (column label)\n",
    "    B = dict()    # same keys as V; this stores the \"backpointers\" to remember best tag sequence\n",
    "    tag_list = sentence_start.keys()\n",
    "    for t in tag_list:\n",
    "        V[(t,0)] = initP(t)*P(sentence[0],t)\n",
    "    for i in range(1,len(sentence)):\n",
    "        for t in tag_list:\n",
    "            pair = argmax(V,tag_list,t,i)\n",
    "            B[(t,i)] = pair[0]\n",
    "            V[(t,i)] = pair[1]*P(sentence[i],t)\n",
    "    printV(sentence,tag_list,V,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note how best tag for past changes to (correct) ADP when we see \"the\" at i=8\n",
    "viterbi('like one little flat near work well past the last right turn'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viterbi('the beer was a little flat'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viterbi('difficulties like high interest rates'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi('banks like high interest rates'.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
